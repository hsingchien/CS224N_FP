{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = {\n",
    "    \"sst_train\": \"data/ids-sst-train.csv\",\n",
    "    \"sst_dev\": \"data/ids-sst-dev.csv\",\n",
    "    \"sst_test\": \"data/ids-sst-test-student.csv\",\n",
    "    \"para_train\": \"data/quora-train.csv\",\n",
    "    \"para_dev\": \"data/quora-dev.csv\",\n",
    "    \"para_test\": \"data/quora-test-student.csv\",\n",
    "    \"sts_train\": \"data/sts-train.csv\",\n",
    "    \"sts_dev\": \"data/sts-dev.csv\",\n",
    "    \"sts_test\": \"data/sts-test-student.csv\",\n",
    "    \"seed\": 11711,\n",
    "    \"epochs\": 10,\n",
    "    \"fine-tune-mode\": \"last-linear-layer\",\n",
    "    \"use_gpu\": \"store_true\",\n",
    "    \"sst_dev_out\": \"predictions/sst-dev-output.csv\",\n",
    "    \"sst_test_out\": \"predictions/sst-test-output.csv\",\n",
    "    \"para_dev_out\": \"predictions/para-dev-output.csv\",\n",
    "    \"para_test_out\": \"predictions/para-test-output.csv\",\n",
    "    \"sts_dev_out\": \"predictions/sts-dev-output.csv\",\n",
    "    \"sts_test_out\": \"predictions/sts-test-output.csv\",\n",
    "    \"batch_size\": 16,\n",
    "    \"hidden_dropout_prob\": 0.3,\n",
    "    \"lr\": 1e-5,\n",
    "}\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 train examples from data/ids-sst-train.csv\n",
      "Loaded 283003 train examples from data/quora-train.csv\n",
      "Loaded 6040 train examples from data/sts-train.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import (\n",
    "    SentenceClassificationDataset,\n",
    "    SentenceClassificationTestDataset,\n",
    "    SentencePairDataset,\n",
    "    SentencePairTestDataset,\n",
    "    load_multitask_data,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "sst_train_path = \"data/ids-sst-train.csv\"\n",
    "para_train_path = \"data/quora-train.csv\"\n",
    "sts_train_path = \"data/sts-train.csv\"\n",
    "sst_train_data, num_labels, para_train_data, sts_train_data = load_multitask_data(\n",
    "    sst_train_path, para_train_path, sts_train_path, split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_train_data = SentenceClassificationDataset(sst_train_data, args)\n",
    "sst_train_dataloader = DataLoader(\n",
    "    sst_train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=sst_train_data.collate_fn,\n",
    ")\n",
    "para_train_data = SentencePairDataset(para_train_data, args)\n",
    "para_train_dataloader = DataLoader(\n",
    "    para_train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=para_train_data.collate_fn,\n",
    ")\n",
    "sts_train_data = SentencePairDataset(sts_train_data, args)\n",
    "sts_train_dataloader = DataLoader(\n",
    "    sts_train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=sts_train_data.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sts_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.word_embedding.weight: torch.Size([30522, 768])\n",
      "bert.pos_embedding.weight: torch.Size([512, 768])\n",
      "bert.tk_type_embedding.weight: torch.Size([2, 768])\n",
      "bert.embed_layer_norm.weight: torch.Size([768])\n",
      "bert.embed_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.0.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.0.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.0.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.0.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.0.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.0.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.0.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.0.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.0.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.0.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.0.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.0.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.0.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.0.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.0.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.0.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.1.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.1.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.1.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.1.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.1.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.1.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.1.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.1.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.1.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.1.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.1.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.1.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.1.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.1.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.1.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.1.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.2.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.2.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.2.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.2.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.2.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.2.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.2.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.2.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.2.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.2.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.2.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.2.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.2.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.2.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.2.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.2.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.3.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.3.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.3.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.3.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.3.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.3.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.3.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.3.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.3.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.3.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.3.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.3.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.3.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.3.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.3.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.3.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.4.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.4.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.4.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.4.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.4.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.4.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.4.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.4.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.4.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.4.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.4.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.4.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.4.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.4.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.4.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.4.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.5.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.5.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.5.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.5.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.5.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.5.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.5.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.5.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.5.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.5.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.5.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.5.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.5.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.5.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.5.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.5.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.6.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.6.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.6.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.6.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.6.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.6.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.6.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.6.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.6.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.6.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.6.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.6.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.6.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.6.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.6.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.6.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.7.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.7.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.7.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.7.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.7.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.7.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.7.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.7.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.7.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.7.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.7.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.7.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.7.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.7.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.7.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.7.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.8.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.8.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.8.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.8.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.8.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.8.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.8.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.8.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.8.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.8.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.8.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.8.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.8.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.8.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.8.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.8.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.9.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.9.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.9.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.9.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.9.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.9.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.9.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.9.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.9.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.9.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.9.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.9.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.9.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.9.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.9.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.9.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.10.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.10.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.10.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.10.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.10.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.10.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.10.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.10.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.10.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.10.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.10.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.10.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.10.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.10.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.10.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.10.out_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.11.self_attention.query.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.11.self_attention.query.bias: torch.Size([768])\n",
      "bert.bert_layers.11.self_attention.key.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.11.self_attention.key.bias: torch.Size([768])\n",
      "bert.bert_layers.11.self_attention.value.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.11.self_attention.value.bias: torch.Size([768])\n",
      "bert.bert_layers.11.attention_dense.weight: torch.Size([768, 768])\n",
      "bert.bert_layers.11.attention_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.11.attention_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.11.attention_layer_norm.bias: torch.Size([768])\n",
      "bert.bert_layers.11.interm_dense.weight: torch.Size([3072, 768])\n",
      "bert.bert_layers.11.interm_dense.bias: torch.Size([3072])\n",
      "bert.bert_layers.11.out_dense.weight: torch.Size([768, 3072])\n",
      "bert.bert_layers.11.out_dense.bias: torch.Size([768])\n",
      "bert.bert_layers.11.out_layer_norm.weight: torch.Size([768])\n",
      "bert.bert_layers.11.out_layer_norm.bias: torch.Size([768])\n",
      "bert.pooler_dense.weight: torch.Size([768, 768])\n",
      "bert.pooler_dense.bias: torch.Size([768])\n",
      "sentiment_af.weight: torch.Size([5, 768])\n",
      "sentiment_af.bias: torch.Size([5])\n",
      "predict_paraphrase_af.weight: torch.Size([2, 1536])\n",
      "predict_paraphrase_af.bias: torch.Size([2])\n",
      "predict_similarity_af.weight: torch.Size([1, 1536])\n",
      "predict_similarity_af.bias: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from multitask_classifier import MultitaskBERT\n",
    "# Init model.\n",
    "config = {\n",
    "    \"hidden_dropout_prob\": 0.3,\n",
    "    \"num_labels\": 3,\n",
    "    \"hidden_size\": 768,\n",
    "    \"data_dir\": \".\",\n",
    "    \"fine_tune_mode\": 'full-model',\n",
    "}\n",
    "\n",
    "config = SimpleNamespace(**config)\n",
    "\n",
    "model = MultitaskBERT(config)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002653837203979492\n",
      "0.00029659271240234375\n",
      "torch.Size([393])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "a = torch.randn((768,768))\n",
    "b = torch.randn((768,768))\n",
    "t = time.time()\n",
    "d = torch.diag(a@b.T)\n",
    "print(f\"{time.time() - t}\")\n",
    "t = time.time()\n",
    "d = torch.bmm(a.unsqueeze(1),b.unsqueeze(-1)).flatten()\n",
    "print(f\"{time.time() - t}\")\n",
    "\n",
    "cflct_idx = torch.nonzero(d<0, as_tuple=True)[0]\n",
    "gradi, gradj = a, b\n",
    "csim = d\n",
    "m = csim[cflct_idx]/(torch.linalg.vector_norm(gradj[cflct_idx,],ord=2)**2)\n",
    "gradi[cflct_idx,] = gradi[cflct_idx,] - gradj[cflct_idx,]/m.unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = [None,None]\n",
    "b = [k for k,aa in enumerate(a) if isinstance(aa, int)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "l1 = torch.nn.Linear(10,2)\n",
    "l2 = torch.nn.Linear(10,3)\n",
    "a = torch.randn(3,10)\n",
    "b = torch.randn(3,10)\n",
    "c = torch.randn(3,2)\n",
    "d = torch.randn(3,3)\n",
    "cp = l1(a)\n",
    "dp = l2(b)\n",
    "loss1 = torch.nn.functional.mse_loss(cp, c,reduction=\"mean\")\n",
    "loss2 = torch.nn.functional.mse_loss(dp,d,reduction=\"mean\")\n",
    "loss = torch.stack((loss1, loss2),dim=0)\n",
    "# loss1.backward()\n",
    "# loss2.backward()\n",
    "sloss = torch.sum(loss)\n",
    "sloss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight grad = tensor([[-0.5569, -0.0263, -0.5904, -0.2771, -0.5700,  0.3826,  1.6349, -0.6649,\n",
      "         -0.0971, -1.3702],\n",
      "        [ 0.0989,  2.2668,  0.3431,  2.2672, -0.0566, -1.7436, -3.5019,  0.1796,\n",
      "         -1.5225,  2.9598]])\n",
      "bias grad = tensor([ 0.4695, -0.1576])\n",
      "weight grad = tensor([[-0.1889, -0.2602,  0.0242, -0.3457,  0.8896, -0.4121, -0.0778,  0.5638,\n",
      "          0.4857,  0.2158],\n",
      "        [-0.1767, -0.5806,  0.0578,  0.0602,  0.8510, -0.5595,  0.2778,  0.7785,\n",
      "          0.6469,  0.0253],\n",
      "        [-0.2446, -1.0186,  0.2515,  0.5808,  0.3606, -0.3409, -0.1084,  0.0341,\n",
      "          0.2340,  0.0353]])\n",
      "bias grad = tensor([-0.0049, -0.3456, -0.4659])\n"
     ]
    }
   ],
   "source": [
    "for name,param in l1.named_parameters():\n",
    "    print(f\"{name} grad = {param.grad}\")\n",
    "for name,param in l2.named_parameters():\n",
    "    print(f\"{name} grad = {param.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n_dfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
