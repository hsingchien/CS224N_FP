# Efficient multi-task learning strategies for single BERT
This is the [default final project](https://github.com/amahankali10/CS224N-Spring2024-DFP-Student-Handout) of Stanford CS224n (2024). In this project we fine-tuned the 110M parameter base BERT model ([`bert-base-uncased`](https://huggingface.co/google-bert/bert-base-uncased)) for 3 downstream tasks: sentiment analysis (SST), paraphrase detection (Para) and semantic textual similarity score (STS). We benchmarked a variety of multi-task fine-tuning techniques, including siamese network design, [`PCGrad`](https://arxiv.org/abs/2001.06782) and [`FAMO`](https://arxiv.org/abs/2306.03792). We also implemented our own variant of `PCGrad`. We also followed [`SBERT`](https://arxiv.org/abs/1908.10084) and found cosine similarity loss on average sentence embedding dramatically improved the performance of STS. We extensively experimented the cosine similarity loss under different settings and analyzed its effects on the token embedding of the base BERT model, which eventually helped us explain the mechanisms behind the STS performance improvement. Lastly, our analysis also highlighted the 'black box' nature of this 100M parameter language model. Unlike the classic [GloVe](https://nlp.stanford.edu/projects/glove/) or [Word2Vec](https://arxiv.org/abs/1301.3781), the token embeddings from this pretrained BERT model are largely uninterpretable, which poses important ethic questions regarding to safeguard the behaviors of these models. The final report pdf will be coming soon! This project was done by Jiamin Sun and Xingjian Zhang.